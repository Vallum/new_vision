# New Vision Model
Hierarchical Machine Vision in complex modality

- In this talk, we are going to explore recent vision AI trends related to complex modal architecture.
- Starting from Deep Learning revolution, via comparative study with animal vision, through Vision-Language multi-modal
- We are going to revisit the understanding of vision. 

### Deep Learning
Everybody knows deep learning.

### So, what is deep learning?

Artificial Neural Networks

Convoultional Neural Networks

![image](https://github.com/Vallum/new_vision/assets/30591790/c68d0b88-beab-4934-bedc-80e4590afbe0)

Attention
Transformers

![image](https://github.com/Vallum/new_vision/assets/30591790/f3c306c3-9555-4342-b9c4-aaa61f667167)

"Attention is All you need"

(BERT and GPT)

![image](https://github.com/Vallum/new_vision/assets/30591790/cd5cbb9b-03d2-4301-a3eb-5b1fe97a3b7c)

"Improving Language Understanding by GenerativePre-Training" GPT-1

GPT-2, GPT-3, ChatGPT

Deep learning is 'interpreting' process of data.

### Data and Language

### Visual Processing and Hierarchy in Science
![image](https://github.com/Vallum/new_vision/assets/30591790/450dc1d4-f03d-4c78-8eb4-61689015e67f)

Complicated and elaborate appratus, but Low level and Mid level

"We can replace much of them with CNN." They believe.

![image](https://github.com/Vallum/new_vision/assets/30591790/2f9cd41c-ae47-439a-910f-b8c4ca975f0b)

This is higher level vision with many modules.

Their functions are various, but anatomically they are homogeneous.

Cortical columns?

Nonetheless, there are entangled hierarchies.

![image](https://github.com/Vallum/new_vision/assets/30591790/103c2bbc-6f83-496c-9a28-71a9c536856f)
![image](https://github.com/Vallum/new_vision/assets/30591790/f7dd523b-5da6-4e91-867d-6953fc8146a6)

But, do we understand higher level concepts by combining lower level concepts?

Classical vision learning seems to believe so.

### Visual Understanding in context

- You can see as much as you know
  
![image](https://github.com/Vallum/new_vision/assets/30591790/1df509e9-36e5-4723-ab24-771c39a93aec)

Do dogs differenciate a tiger and a stuffed animal?

![image](https://github.com/Vallum/new_vision/assets/30591790/55858ea9-afba-4978-8399-5b6772c112bd)

Do they even know about real tigers?

Do they really have the concept?

Apparently they have some fear about unknown monsters in their DNA and brain-wiring from the natural selection.

If their visual resolution is not high enough and if they have extended analogical ability, they could imagine it as a some live predator.

Eyes, ears, legs, args, and tooth with claws

![image](https://github.com/Vallum/new_vision/assets/30591790/7d021750-babd-437d-a700-523dab2fa1e2)

Howeve, the bisons are indifferent of cars.

Do they know what a car is?

![image](https://github.com/Vallum/new_vision/assets/30591790/61bf9c6c-3582-4736-b55d-a792c1e3c2f9)

To understand this as a car, what do we know as background knowdledge?

Four wheels, fuels, and the history of carriage through internal combustion engines?

![image](https://github.com/Vallum/new_vision/assets/30591790/14d5cf93-8084-46e4-8fa8-06b9e491decf)

To understand this as an airplane, ...

However, for Deep Learning, we usaully do not provide those kinds of knowleges to networks.

Instead, we just need many pictures.

### Vision in DL without descrptive interpretation

With a million images,

Without description, however there is always some context.

With class signature, a little bit of semiotics, but not with syntax or bag of words

- ImageNet 21K categories

- ImageNet 1000 categories

![image](https://github.com/Vallum/new_vision/assets/30591790/764bb2c8-8056-4184-8e56-268f1df6f894)

![image](https://github.com/Vallum/new_vision/assets/30591790/0ba191c3-94b1-4265-b3be-83d44c730882)

| Class ID | Class Name                                                                                        |
|----------|---------------------------------------------------------------------------------------------------|
| 0        | tench, Tinca tinca                                                                                        |
| 1        | goldfish, Carassius auratus                                                                            |
| 2        | great white shark, white shark, man-eater, man-eating shark, Carcharodon caharias',                  |
| 3        | tiger shark, Galeocerdo cuvieri                                                                   |
| 4        | hammerhead, hammerhead shark                                                                        |
| 5        | electric ray, crampfish, numbfish, torpedo                                                        |
| 6        | stingray                                                                                         |
| 7        | cock                                                                                             |
| 8        | hen                                                                                               |
| 9        | ostrich, Struthio camelus                                                                           |
| 10       | brambling, Fringilla montifringilla                                                                |
| 11       | goldfinch, Carduelis carduelis                                                                     |
| 12       | house finch, linnet, Carpodacus mexicanus                                                      |
| 13       | junco, snowbird                                                                                   |
| 14       | indigo bunting, indigo finch, indigo bird, Passerina cyanea                                       |
| 15       | robin, American robin, Turdus migratorius    |                   


The entities are from WordNet
![image](https://github.com/Vallum/new_vision/assets/30591790/cf05aaac-2c2f-48d1-88e1-cb1c8446ab38)
(https://www.cs.princeton.edu/courses/archive/spring11/cos226/assignments/wordnet.html)

Can we understand visual information only with visual information?

Never. We have already been dependent on semiotics

### Vision in DL with descrptive interpretation

With Language with bag of words and little syntax

![image](https://github.com/Vallum/new_vision/assets/30591790/04b5ec2b-3b14-4469-b94d-596c1bafb69d)

(Open AI, CLIP)

They are much better with abundent description.

### With Language with syntax

With Large Language models such as GPT-{3,4}, ChatGPT-{3,4}

![image](https://github.com/Vallum/new_vision/assets/30591790/9ff702aa-8021-4609-8496-a2e15fd0f74d)

### UnClip

Dalle-2,3

![image](https://github.com/Vallum/new_vision/assets/30591790/d3d1799c-4beb-472e-8bf7-86682d3384ee)

![image](https://github.com/Vallum/new_vision/assets/30591790/3571161d-958d-4e12-8be4-ff59bc31f33b)

![image](https://github.com/Vallum/new_vision/assets/30591790/679f5ca3-fae2-4912-913a-cbfad283eac9)

HierarchicalText-Conditional ImageGenerationwithCLIPLatents


### Image as a language

![image](https://github.com/Vallum/new_vision/assets/30591790/6986eaa6-4421-4c9a-b632-b12827b864bc)

Image as a foreign language

![image](https://github.com/Vallum/new_vision/assets/30591790/e442693c-7c11-4128-96fd-eae339adbf38)

Prompt is Context

![image](https://github.com/Vallum/new_vision/assets/30591790/621941d5-8241-405b-b641-9f91b29a3c68)

![image](https://github.com/Vallum/new_vision/assets/30591790/a9d65828-e083-42e8-a519-822e41f2bd83)

Remember the homogenuity of neural anatomy. 

Overall, machine vision is a task of photo(image) description.

### What if the subject has no language to describe 4-D world?

![image](https://github.com/Vallum/new_vision/assets/30591790/4626be98-f0bd-4b5a-af5d-0c3cdfad8d31)

![image](https://github.com/Vallum/new_vision/assets/30591790/75c1d73e-783e-486a-a36e-4b8e371ea178)

Can they descriptively model the world in their brains?

### What if the object shows only part of the 4-D world?

Do Vision AIs perceive this as a car when they have unseen it? 

Yes, maybe. But...

To describe this as a 180 degree rear view of some car?

![image](https://github.com/Vallum/new_vision/assets/30591790/e9573486-6c2b-446f-9a69-db493894fb4b)

### Yet Again Physical Reality

- Perceiving space and location

![image](https://github.com/Vallum/new_vision/assets/30591790/de6bcec7-dcdb-4307-a2d8-8ae2a2f53352)

(OpenImages)

- And 3-D Perception

- With anatomical homogeneous modules.

### Conclusion

- Vision task is a description of electro-magnetic signal in language or in some semiotic form,
- via object entitiy modeling(or understanding).
